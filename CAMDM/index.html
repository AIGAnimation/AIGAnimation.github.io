<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Deformable Neural Radiance Fields creates free-viewpoint portraits (nerfies) from casually captured videos.">
  <meta name="keywords" content="Nerfies, D-NeRF, NeRF">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Taming Diffusion Probabilistic Models for Character Control</title>

  <script async src="https://www.googletagmanager.com/gtag/js?id=G-NLE0V1WZWF"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-NLE0V1WZWF');
  </script>
  
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">


  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>

</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Taming Diffusion Probabilistic Models for Character Control</h1>
          <p style="padding-bottom: 10px;">
            <span class="is-size-5" style="font-weight: bold;">Siggraph 2024</span>
            <p>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://aruichen.github.io/">Rui Chen</a><sup>*</sup><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://rubbly.cn/">Mingyi Shi</a><sup>*</sup><sup>2</sup>,</span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=o31BPFsAAAAJ&hl=en">Shaoli Huang</a><sup>3</sup>,
            </span>
            <span class="author-block">
              <a href="https://ece.hkust.edu.hk/pingtan">Ping Tan</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://scholar.google.com.hk/citations?user=TApLOhkAAAAJ&hl=en">Taku Komura</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://xuelin-chen.github.io/">Xuelin Chen</a><sup>3</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span style="font-size: 18px;" class="footnote"><sup>*</sup>Equal contribution</span>
            <div class="is-size-6 publication-authors">
              <span class="author-block"><sup>1 </sup>Hong Kong University of Science and Technology,</span>
              <span class="author-block"><sup>2 </sup>The University of Hong Kong,</span>
              <span class="author-block"><sup>3 </sup>Tencent AI Lab</span>
            </div>
            
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <span class="link-block">
                <a href="https://arxiv.org/abs/2404.15121"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>ArXiv</span>
                </a>
              </span>
              <!-- PDF Link. -->
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/AIGAnimation/CAMDM"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code (comming soon)</span>
                  </a>
              </span>
              <span class="link-block">
                <a href="https://drive.google.com/file/d/1NYXP-fbEegErfaIgtHXvvrrfLXUSqYXg/view?usp=sharing"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Unity demo</span>
                  </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section" style="background-color: #f5f5f5;">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            We present a novel character control framework that effectively utilizes motion diffusion probabilistic models to generate high-quality and diverse character animations, responding in real-time to a variety of dynamic user-supplied control signals. 
            At the heart of our method lies a transformer-based Conditional Autoregressive Motion Diffusion Model (CAMDM), which takes as input the character's historical motion and can generate a range of diverse potential future motions conditioned on high-level, coarse user control.
            To meet the demands for diversity, controllability, and computational efficiency required by a real-time controller,
            we incorporate several key algorithmic designs. 
            These include separate condition tokenization, classifier-free guidance on past motion, and heuristic future trajectory extension, all designed to address the challenges associated with taming motion diffusion probabilistic models for character control.
            As a result,
            our work represents the first model
            that enables real-time generation of high-quality, diverse character animations based on user interactive control, supporting animating the character in multiple styles with a single
            unified model. 
            We evaluate our method on a diverse set of locomotion skills, demonstrating the merits of our method over existing character controllers.
          </p>
        </div>
      </div>
    </div>
  </div>
    <!--/ Abstract. -->
</section>

<section class="section">
  <div class="container is-max-desktop">

    <!-- Paper video. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Video</h2>
        <div class="publication-video">
          <iframe src="https://www.youtube.com/embed/ThTohSITmWA?rel=0&amp;modestbranding=1"
                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
        </div>
      </div>
    </div>
    <!--/ Paper video. -->
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">  
    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Method</h2>
        <div class="content has-text-justified">
          <p>
            Given a large-scale locomotion dataset 100STYLE,
            our method first trains a Conditional Autoregressive Motion Diffusion Model (CAMDM), which
            takes as input the past motion of F frames of the character, and user control parameters, and then learns to capture the conditional distribution of the future motions x (of F frames).
            During runtime, CAMDM is applied at each frame with the on-the-fly collected character's historical poses, user control inputs, and randomly sampled Gaussian noise, and then sample from the conditional motion distribution, obtaining a sequence of realistic future postures to be displayed.
            The character animated using this autoregressive generation approach can exhibit coherent and diverse motions while adhering to user inputs. 
            This is achieved because x is trained to capture all possible future motions under different conditions, yielding a wide range of plausible animations.
          </p>
        </div> 
        <img style="width: 60%;" src="assets/pipeline.png">
      </div>
    </div>
  </div>   
</section>  

<section class="section" style="background-color: #f5f5f5;">
  <div class="container is-max-desktop has-text-centered">
    <h2 style="padding-top: 20px;" class="title is-3">Results</h2>
    <!-- <h2 class="title is-3">Results</h2> -->
    <div class="columns is-centered ">
      <div class="column">
        <div class="content">
          <h2 class="title is-5">Natural Transition</h2>
          <p>
            Our method produce natural and smooth transition between different styles of motion.
          </p>
          <video  controls muted loop playsinline height="100%">
            <source src="./assets/videos/result_transition_compression.mp4" type="video/mp4">
          </video>
        </div>
      </div>
      <!--/ Visual Effects. -->

      <!-- Matting. -->
      <div class="column">
        <h2 class="title is-5">Robust Control</h2>
        <div class="columns is-centered">
          <div class="column content">
            <p>
              Our method can generate robust and high-quality animations based on user-applied control.
            </p>
            <video  controls loop playsinline height="100%">
              <source src="./assets/videos/result_robust_compression.mp4"
                    type="video/mp4">
            </video>
          </div>
        </div>
      </div>
    </div>

    <h2 style="padding-top: 20px;" class="title is-5">Variability within Styles</h2>
    <p class="column content">
      Our method is a probabilistic diffusion model, rooted in the denoising process of random noise that 
      yields different results even when provided with different conditional inputs. 
      The method is adept at generating diverse intra-style motions, offering a variability of movements 
      within each style while effectively circumventing the 'mean pose' issue. 
    </p>
    
    <div class="columns is-centered ">
      <div class="column">
        <div class="content">
         
          <video  controls muted loop playsinline height="100%">
            <source src="./assets/videos/result_diversity_waving_compression.mp4" type="video/mp4">
          </video>
        </div>
      </div>
      <!--/ Visual Effects. -->

      <!-- Matting. -->
      <div class="column">
        <div class="columns is-centered">
          <div class="column content">
            <video  controls loop playsinline height="100%">
              <source src="./assets/videos/result_diversity_drunk_compression.mp4"
                    type="video/mp4">
            </video>
          </div>
        </div>
      </div>
    </div>

  </div>
</section>

<section class="section">
  <div class="container is-max-desktop has-text-centered">
    <h2 style="padding-top: 20px;" class="title is-3">Ablation</h2>
    <p class="column content">
      We conduct ablation studies to evaluate the effectiveness of our proposed designs.
    
      (1) <b>Multi-token</b> : The system, when functioning without a multi-token design, tends to generate more artifacts and exhibits subpar control. 
      (2) <b>CFG on past motion</b> : When compared to the default CFG integrated with past motion, as utilized in our method, the application of CFG on the conditional style label results in a less desirable style transition.
      <br>For more ablation on HFTE/Diffusion steps/Backbone, etc, please refer to our paper.
    </p>
    <div class="columns is-centered ">
      <div class="column">
        <div class="content">
         
          <video  controls muted loop playsinline height="100%">
            <source src="./assets/videos/ablation1_compression.mp4" type="video/mp4">
          </video>
          <p style="padding-top: 10px;">w/o vs. w multi-token</p>
        </div>
      </div>
      <!--/ Visual Effects. -->

      <!-- Matting. -->
      <div class="column">
        <div class="columns is-centered">
          <div class="column content">
            <video  controls loop playsinline height="100%">
              <source src="./assets/videos/ablation2_compression.mp4"
                    type="video/mp4">
              
            </video>
            <p style="padding-top: 10px;">w/o vs. w CFG on past motion</p>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section" id="BibTeX" >
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>
      @InProceedings{chen2024taming,
        title={Taming Diffusion Probabilistic Models for Character Control},
        author={Rui Chen and Mingyi Shi and Shaoli Huang and Ping Tan and Taku Komura and Xuelin Chen},
        year={2024},
        eprint={2404.15121},
        archivePrefix={arXiv},
        primaryClass={cs.GR}
    }
      </code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <!-- <div class="column is-8"> -->
        <div class="content">
          <p>
            This website is constructed using the source code provided by <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>, 
            allow us to express our appreciation for their contribution.                              
          </p>
    
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
